</path/to/saved/keypair/file.pem>
/Users/yasha.podeswa/.ssh/aws-personal-uswest-oregon.pem

<master.public-dns-name.amazonaws.com>
ec2-54-186-114-154.us-west-2.compute.amazonaws.com

# Standard Hadoop Connection
ssh -o "ServerAliveInterval 10" -i ~/.ssh/aws-personal-uswest-oregon.pem hadoop@ec2-54-186-114-154.us-west-2.compute.amazonaws.com
# With basic browser monitoring
ssh -L 9100:localhost:9100 -L 9101:localhost:9101  -i ~/.ssh/aws-personal-uswest-oregon.pem hadoop@ec2-54-186-114-154.us-west-2.compute.amazonaws.com
# Then open in browser:
http://localhost:9101
# Or, for the SOCKS setup
ssh -o "ServerAliveInterval 10" -i /Users/yasha.podeswa/.ssh/aws-personal-uswest-oregon.pem -ND 8888 hadoop@ec2-54-186-114-154.us-west-2.compute.amazonaws.com
# Then open in FIREFOX
# Job tracker
http://ec2-54-186-114-154.us-west-2.compute.amazonaws.com:9100/
# HDFS management
http://ec2-54-186-114-154.us-west-2.compute.amazonaws.com:9101/

# Start pig in interactive mode
pig

########
# ALL OF THE FOLLOWING IN GRUNT:
########
# Create the "/user/hadoop" Directory in the Hadoop Filesystem
fs -mkdir /user/hadoop
# List files in this directory
fs -ls /user/hadoop
# To exit pig
quit

# Copy from Hadoop file system to server's standard file system
# This way will get it to you in the different chunks:
hadoop fs -copyToLocal /user/hadoop/example-results example-results
# While this way will merge the chunks into a single file <-- generally better
hadoop fs -getmerge  /user/hadoop/example-results example-results

# FROM LOCAL MACHINE, to copy files back:
# Single file:
scp -o "ServerAliveInterval 10" -i /Users/yasha.podeswa/.ssh/aws-personal-uswest-oregon.pem hadoop@ec2-54-186-114-154.us-west-2.compute.amazonaws.com:/home/hadoop/example-results .
# Copy entire directory recursively
scp -o "ServerAliveInterval 10" -i /Users/yasha.podeswa/.ssh/aws-personal-uswest-oregon.pem -r hadoop@ec2-54-186-114-154.us-west-2.compute.amazonaws.com:/home/hadoop/example-results .

########
# ALL OF THE FOLLOWING IN GRUNT:
########

# 1.1 How many MapReduce jobs are generated by example.pig?
3

# 1.2 How many reduce tasks are within the first MapReduce job? How many reduce tasks are within later MapReduce jobs?
Task 1: 33 map jobs, 50 reduce jobs
Task 2: 1 map jobs, 1 reduce jobs
Task 3: 1 map jobs, 50 reduce jobs

# 1.3 How long does each job take? How long does the entire script take?
~2 mins per job, ~6 mins total

# 1.4 What is the schema of the tuples after each of the following commands in example.pig?
# After the command ntriples = ...
grunt> describe ntriples;
ntriples: {subject: chararray,predicate: chararray,object: chararray}
# After the command objects = ...
grunt> describe objects;
objects: {group: chararray,ntriples: {(subject: chararray,predicate: chararray,object: chararray)}}
# After the command count_by_object = ...
grunt> describe count_by_object;
count_by_object: {group: chararray,count: long}

# What you need to turn in:
# How many records are there in count_by_object? DON'T FORGET TO SHUTDOWN YOUR INSTANCES!
Is it...
1622294
That's the number of lines in the final file --> is that what I want?


